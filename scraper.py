"""
Madam Coco √úr√ºn √áekme Scripti
Selenium ile t√ºm √ºr√ºnleri √ßeker ve CSV'ye kaydeder
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import pandas as pd
import time
from datetime import datetime
import os

class MadamCocoScraper:
    def __init__(self):
        """Selenium ba≈ülat"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.binary_location = '/usr/bin/chromium-browser'
        
        service = Service('/usr/bin/chromedriver')
        self.driver = webdriver.Chrome(service=service, options=options)
        self.products = []
    
    def scrape_category(self, category_url, max_pages=5):
        """Kategoriden √ºr√ºnleri √ßek"""
        print(f"\nüîç Kategori taranƒ±yor: {category_url}")
        
        for page in range(1, max_pages + 1):
            url = f"{category_url}?page={page}"
            print(f"\nüìÑ Sayfa {page} √ßekiliyor...")
            
            try:
                self.driver.get(url)
                time.sleep(3)
                
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                products_found = self.extract_products()
                
                if not products_found:
                    print(f"‚ö†Ô∏è Sayfa {page}'de √ºr√ºn bulunamadƒ±, durduruluyor")
                    break
                
                print(f"‚úÖ {len(products_found)} √ºr√ºn √ßekildi")
                
            except Exception as e:
                print(f"‚ùå Sayfa {page} hatasƒ±: {e}")
                break
        
        print(f"\nüéâ Toplam {len(self.products)} √ºr√ºn √ßekildi!")
    
    def extract_products(self):
        """Sayfadaki √ºr√ºnleri √ßƒ±kar"""
        found = []
        
        selectors = [
            "div.product-item",
            "div.product-card",
            "article.product",
            "div[data-product-id]",
            ".product"
        ]
        
        for selector in selectors:
            try:
                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"üì¶ {len(elements)} √ºr√ºn bulundu ({selector})")
                    
                    for element in elements:
                        product = self.parse_product(element)
                        if product:
                            self.products.append(product)
                            found.append(product)
                    
                    return found
            except:
                continue
        
        return found
    
    def parse_product(self, element):
        """Tek √ºr√ºn√º parse et"""
        try:
            name = None
            name_selectors = ["h3", "h2", ".product-name", ".product-title", "a"]
            for sel in name_selectors:
                try:
                    name = element.find_element(By.CSS_SELECTOR, sel).text.strip()
                    if name:
                        break
                except:
                    continue
            
            price = None
            price_selectors = [".price", ".product-price", "span.price-value"]
            for sel in price_selectors:
                try:
                    price_text = element.find_element(By.CSS_SELECTOR, sel).text
                    price = self.parse_price(price_text)
                    if price:
                        break
                except:
                    continue
            
            link = None
            try:
                link = element.find_element(By.TAG_NAME, "a").get_attribute("href")
            except:
                pass
            
            image = None
            try:
                img = element.find_element(By.TAG_NAME, "img")
                image = img.get_attribute("src") or img.get_attribute("data-src")
            except:
                pass
            
            if name and price:
                return {
                    '√úr√ºn Adƒ±': name,
                    'Fiyat': price,
                    'Link': link,
                    'G√∂rsel': image,
                    'Tarih': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
        
        except Exception as e:
            pass
        
        return None
    
    def parse_price(self, price_text):
        """Fiyatƒ± sayƒ±ya √ßevir"""
        import re
        if not price_text:
            return None
        
        clean = re.sub(r'[^\d,.]', '', price_text)
        clean = clean.replace('.', '').replace(',', '.')
        
        try:
            return float(clean)
        except:
            return None
    
    def save_to_csv(self, filename='output/madamcoco_products.csv'):
        """CSV'ye kaydet"""
        if not self.products:
            print("‚ö†Ô∏è Kaydedilecek √ºr√ºn yok!")
            return
        
        df = pd.DataFrame(self.products)
        
        os.makedirs('output', exist_ok=True)
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nüíæ {len(self.products)} √ºr√ºn kaydedildi: {filename}")
        
        print(f"\nüìä ƒ∞lk 5 √ºr√ºn:")
        print(df.head())
    
    def close(self):
        """Tarayƒ±cƒ±yƒ± kapat"""
        self.driver.quit()


if __name__ == "__main__":
    print("üõçÔ∏è MADAM COCO SCRAPER")
    print("=" * 50)
    
    scraper = MadamCocoScraper()
    
    categories = [
        "https://www.madamcoco.com.tr/ev-tekstili"
    ]
    
    for category in categories:
        scraper.scrape_category(category, max_pages=3)
    
    scraper.save_to_csv()
    
    scraper.close()
    
    print("\n‚úÖ ƒ∞≈ülem tamamlandƒ±!")
